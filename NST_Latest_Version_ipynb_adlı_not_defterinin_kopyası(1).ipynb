{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BatuhanSimsk/BatuhanSimsk/blob/main/NST_Latest_Version_ipynb_adl%C4%B1_not_defterinin_kopyas%C4%B1(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Imports*"
      ],
      "metadata": {
        "id": "Q2USTSiNAdx7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aaQYNrWe6L6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8786ac1f-6e98-4b7d-d016-6a9745a4ea12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.31.4-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.16.4 (from gradio)\n",
            "  Downloading gradio_client-0.16.4-py3-none-any.whl (315 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.9/315.9 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.4.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Collecting typer<1.0,>=0.12 (from gradio)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.16.4->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.16.4->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->gradio)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->gradio)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->gradio)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=614a6a20c69fdae1f53520a41d724cbb202738b73c3eb315ef2dfcc3ca407212\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uvloop, ujson, tomlkit, shellingham, semantic-version, ruff, python-multipart, python-dotenv, orjson, httptools, h11, dnspython, aiofiles, watchfiles, uvicorn, starlette, httpcore, email_validator, typer, httpx, gradio-client, fastapi-cli, fastapi, gradio\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 gradio-4.31.4 gradio-client-0.16.4 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 orjson-3.10.3 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 ruff-0.4.4 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.37.2 tomlkit-0.12.0 typer-0.12.3 ujson-5.10.0 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install gradio\n",
        "\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *VGG19 Neural Model Specialization*"
      ],
      "metadata": {
        "id": "VUC-aBAgAs0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        # The first number x in convx_y gets added by 1 after it has gone\n",
        "        # through a maxpool, and the second y if we have several conv layers\n",
        "        # in between a max pool. These strings (0, 5, 10, ..) then correspond\n",
        "        # to conv1_1, conv2_1, conv3_1, conv4_1, conv5_1 mentioned in NST paper\n",
        "        self.chosen_features = [\"0\", \"5\", \"10\", \"19\", \"28\"]\n",
        "\n",
        "        # We don't need to run anything further than conv5_1 (the 28th module in vgg)\n",
        "        # Since remember, we dont actually care about the output of VGG: the only thing\n",
        "        # that is modified is the generated image (i.e, the input).\n",
        "        self.model = models.vgg19(pretrained=True).features[:29]\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Store relevant features\n",
        "        features = []\n",
        "\n",
        "        # Go through each layer in model, if the layer is in the chosen_features,\n",
        "        # store it in features. At the end we'll just return all the activations\n",
        "        # for the specific layers we have in chosen_features\n",
        "        for layer_num, layer in enumerate(self.model):\n",
        "            x = layer(x)\n",
        "\n",
        "            if str(layer_num) in self.chosen_features:\n",
        "                features.append(x)\n",
        "\n",
        "        return features"
      ],
      "metadata": {
        "id": "oNt5rYIttGFn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Defining Image Upload Function*"
      ],
      "metadata": {
        "id": "nf6mJrvvHFNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "def load_image(image, device):\n",
        "    # Gelen resim numpy dizisi şeklinde olabilir\n",
        "    if isinstance(image, np.ndarray):\n",
        "        # Numpy dizisini PIL.Image olarak aç\n",
        "        image = Image.fromarray(image)\n",
        "    # PIL.Image olarak işlenebilecek bir formatta olmalıdır\n",
        "    if not isinstance(image, Image.Image):\n",
        "        raise ValueError(\"Unsupported image format\")\n",
        "\n",
        "    # PIL.Image'i uygun şekilde işle ve dönüştür\n",
        "    # Burada uygun işlemleri gerçekleştir (örneğin boyut değişimi, normalizasyon vs.)\n",
        "\n",
        "    # Dönüştürülen görüntüyü uygun cihaza taşı\n",
        "    image = loader(image).unsqueeze(0)\n",
        "    return image.to(device)"
      ],
      "metadata": {
        "id": "kDBr4AActLtm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Transforming to a Tensor and Load Tensor to GPU*"
      ],
      "metadata": {
        "id": "NjOIG6GnJ6MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "imsize = 256\n",
        "\n",
        "# Here we may want to use the Normalization constants used in the original\n",
        "# VGG network (to get similar values net was originally trained on), but\n",
        "# I found it didn't matter too much so I didn't end of using it. If you\n",
        "# use it make sure to normalize back so the images don't look weird.\n",
        "loader = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((imsize, imsize)),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "E3faOV0QtSkf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Image Uploading*"
      ],
      "metadata": {
        "id": "ukvKwQ-XKML7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from gradio import components\n",
        "#from gradio.outputs import Image as OutputImage\n",
        "from torchvision.transforms import ToPILImage\n",
        "\n",
        "# NST işlemlerini gerçekleştiren fonksiyon\n",
        "def neural_style_transfer(original_img, style_img, total_steps, learning_rate, alpha, beta):\n",
        "\n",
        "    # Yükleme işlemlerini buraya taşıyabilirsiniz\n",
        "    original_img = load_image(original_img, device)\n",
        "    style_img = load_image(style_img, device)\n",
        "\n",
        "    # Yüklenen resimleri uygun cihaza taşı\n",
        "    #original_img = load_image(original_img, device)\n",
        "    #style_img = load_image(style_img, device)\n",
        "\n",
        "    # Generative image'i oluştururken kullanılacak başlangıç resmini orijinal resimden al\n",
        "    generated = original_img.clone().requires_grad_(True)\n",
        "\n",
        "    # NST modelini çağır ve değerlendirme modunda kullan\n",
        "    model = VGG().to(device).eval()\n",
        "\n",
        "    # Optimizasyon algoritması ve hiperparametreler\n",
        "    #total_steps = 3000\n",
        "    #learning_rate = 0.001\n",
        "    #alpha = 0.01\n",
        "    #beta = 1\n",
        "    optimizer = optim.Adam([generated], lr=learning_rate)\n",
        "\n",
        "    # Kayıp değerlerini saklamak için liste\n",
        "    total_loss_list = []\n",
        "    style_loss_list = []\n",
        "    content_loss_list = []\n",
        "\n",
        "    # NST algoritmasını uygula\n",
        "    for step in tqdm(range(total_steps)):\n",
        "        # Convolution özelliklerini belirli katmanlardan al\n",
        "        generated_features = model(generated)\n",
        "        original_img_features = model(original_img)\n",
        "        style_features = model(style_img)\n",
        "\n",
        "        # Kayıp başlangıçta 0\n",
        "        style_loss = original_loss = 0\n",
        "\n",
        "        # Seçilen katmanlar için tüm özellikleri dolaş\n",
        "        for gen_feature, orig_feature, style_feature in zip(\n",
        "            generated_features, original_img_features, style_features\n",
        "        ):\n",
        "            # batch_size burada 1 olacak\n",
        "            batch_size, channel, height, width = gen_feature.shape\n",
        "            original_loss += torch.mean((gen_feature - orig_feature) ** 2)\n",
        "            # Oluşturulan için Gram Matrisi hesapla\n",
        "            G = gen_feature.view(channel, height * width).mm(\n",
        "                gen_feature.view(channel, height * width).t()\n",
        "            )\n",
        "            # Stil için Gram Matrisi hesapla\n",
        "            A = style_feature.view(channel, height * width).mm(\n",
        "                style_feature.view(channel, height * width).t()\n",
        "            )\n",
        "            style_loss += torch.mean((G - A) ** 2)\n",
        "\n",
        "        total_loss = alpha * original_loss + beta * style_loss\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "    \"\"\"\n",
        "        # Kayıp değerlerini sakla\n",
        "        total_loss_list.append(total_loss.item())\n",
        "        style_loss_list.append(style_loss.item())\n",
        "        content_loss_list.append(original_loss.item())\n",
        "\n",
        "        fig, axs = plt.subplots(3, figsize=(8, 6))\n",
        "\n",
        "        # Toplam kayıp değerlerini grafiğe ekleyin\n",
        "        axs[0].plot(total_loss_list, label='Total Loss')\n",
        "        axs[0].set_xlabel('Step')\n",
        "        axs[0].set_ylabel('Total Loss')\n",
        "        axs[0].set_title('Total Loss Değişimi')\n",
        "        axs[0].legend()\n",
        "\n",
        "        # Style loss değerlerini grafiğe ekleyin\n",
        "        axs[1].plot(style_loss_list, label='Style Loss', color='orange')\n",
        "        axs[1].set_xlabel('Step')\n",
        "        axs[1].set_ylabel('Style Loss')\n",
        "        axs[1].set_title('Style Loss Değişimi')\n",
        "        axs[1].legend()\n",
        "\n",
        "        # Content loss değerlerini grafiğe ekleyin\n",
        "        axs[2].plot(content_loss_list, label='Content Loss', color='green')\n",
        "        axs[2].set_xlabel('Step')\n",
        "        axs[2].set_ylabel('Content Loss')\n",
        "        axs[2].set_title('Content Loss Değişimi')\n",
        "        axs[2].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.savefig(\"plot.png\", format='png')\n",
        "        # Dosyayı PILImage olarak yükle\n",
        "        pil_image = Image.open(\"plot.png\")\n",
        "        # Dosyayı sildikten sonra\n",
        "        os.remove(\"plot.png\")\n",
        "        # Resmi döndürün\n",
        "        return pil_image\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    if step % 250 == 0:\n",
        "        tqdm.write(f\"Step {step}, Total Loss: {total_loss.item()}\")\n",
        "\n",
        "    generated_pil_image = ToPILImage()(generated.squeeze().detach())\n",
        "    generated_pil_image.resize((256,256))\n",
        "    generated_pil_image.save(\"generated_image.jpeg\", format=\"jpeg\")  # Resmi .jpeg formatında kaydedin\n",
        "\n",
        "    # Sonuç olarak üretilen resmi döndür\n",
        "    return generated_pil_image\n",
        "\n",
        "    # Sonuç olarak üretilen resmi döndür\n",
        "    #return generated\n",
        "\n",
        "\n",
        "# Gradio UI'sını oluştur\n",
        "inputs = [\n",
        "\n",
        "    components.Image(label=\"Orijinal Resim\", type=\"pil\", sources=\"upload\"),\n",
        "    components.Image(label=\"Stil Resmi\", type=\"pil\", sources=\"upload\"),\n",
        "    components.Number(label=\"Toplam Adım Sayısı\", value=3000, minimum=500, maximum=10000, step=10),\n",
        "    components.Slider(label=\"Öğrenme Oranı\", value=0.001, minimum=0.0001, maximum=0.01, step=0.0001),\n",
        "    components.Slider(label=\"Alpha\", value=0.01, minimum=0.001, maximum=0.1, step=0.001),\n",
        "    components.Slider(label=\"Beta\", value=1, minimum=0.1, maximum=10, step=0.1)\n",
        "]\n",
        "\n",
        "outputs = [\n",
        "    components.Image(label=\"Üretilen Resim\", type=\"pil\"),\n",
        "    #components.Image(label=\"Grafik\", type=\"pil\"),\n",
        "    ]\n",
        "\n",
        "# UI ile NST fonksiyonunu bağla ve sunucu modunda başlat\n",
        "gr.Interface(\n",
        "    fn=neural_style_transfer,\n",
        "    inputs=inputs,\n",
        "    outputs=outputs,\n",
        "    title=\"Neural Style Transfer\",\n",
        "    description=\"Orijinal resmi ve bir stil resmi yükleyin, sonucu görselleştirin.\",\n",
        ").queue().launch(debug = True, share= True)\n",
        "\n",
        "while True:\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "5fYW8ZSnepDL",
        "outputId": "85ac82fa-6485-4d9d-ac44-1572c9a1f5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://eb4272bd0cd4a4161c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://eb4272bd0cd4a4161c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Orijinal resmi yükle\n",
        "print(\"Lütfen orijinal resmi yükleyin:\")\n",
        "uploaded_original = files.upload()\n",
        "original_img_name = list(uploaded_original.keys())[0]\n",
        "\n",
        "# Stil resmini yükle\n",
        "print(\"\\nLütfen stil resmini yükleyin:\")\n",
        "uploaded_style = files.upload()\n",
        "style_img_name = list(uploaded_style.keys())[0]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# Orijinal ve stil resimlerini belirle\n",
        "original_img = load_image(original_img_name, device)\n",
        "style_img = load_image(style_img_name, device)\n",
        "\n",
        "# Generative image'i oluştururken kullanılacak başlangıç resmini orijinal resimden al\n",
        "generated = original_img.clone().requires_grad_(True)\n",
        "\"\"\"\n",
        "\n",
        "#generated = torch.randn(original_img.shape, device=device, requires_grad=True)"
      ],
      "metadata": {
        "id": "GrmUHlgvtWza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Defining Hyperparameters & Optimizing*"
      ],
      "metadata": {
        "id": "cMjICt3mKQzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = VGG().to(device).eval()\n",
        "\n",
        "# Hyperparameters\n",
        "total_steps = 1500\n",
        "learning_rate = 0.001\n",
        "alpha = 0.01\n",
        "beta = 1\n",
        "optimizer = optim.Adam([generated], lr=learning_rate)"
      ],
      "metadata": {
        "id": "H81er882thy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_loss_list = []  # Toplam kayıp değerlerini saklamak için liste\n",
        "style_loss_list = []  # Style loss değerlerini saklamak için liste\n",
        "content_loss_list = []  # Content loss değerlerini saklamak için liste\n",
        "\n",
        "for step in tqdm(range(total_steps)):\n",
        "    # Obtain the convolution features in specifically chosen layers\n",
        "    generated_features = model(generated)\n",
        "    original_img_features = model(original_img)\n",
        "    style_features = model(style_img)\n",
        "\n",
        "    # Loss is 0 initially\n",
        "    style_loss = original_loss = 0\n",
        "\n",
        "    # iterate through all the features for the chosen layers\n",
        "    for gen_feature, orig_feature, style_feature in zip(\n",
        "        generated_features, original_img_features, style_features\n",
        "    ):\n",
        "\n",
        "        # batch_size will just be 1\n",
        "        batch_size, channel, height, width = gen_feature.shape\n",
        "        original_loss += torch.mean((gen_feature - orig_feature) ** 2)\n",
        "        # Compute Gram Matrix of generated\n",
        "        G = gen_feature.view(channel, height * width).mm(\n",
        "            gen_feature.view(channel, height * width).t()\n",
        "        )\n",
        "        # Compute Gram Matrix of Style\n",
        "        A = style_feature.view(channel, height * width).mm(\n",
        "            style_feature.view(channel, height * width).t()\n",
        "        )\n",
        "        style_loss += torch.mean((G - A) ** 2)\n",
        "\n",
        "    total_loss = alpha * original_loss + beta * style_loss\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss_list.append(total_loss.item())  # Toplam kayıp değerini listeye ekle\n",
        "    style_loss_list.append(style_loss.item())  # Style loss değerini listeye ekle\n",
        "    content_loss_list.append(original_loss.item())  # Content loss değerini listeye ekle\n",
        "\n",
        "    if step % 250 == 0:\n",
        "        tqdm.write(f\"Step {step}, Total Loss: {total_loss.item()}\")\n",
        "        #save_image(generated, \"generated.png\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Toplam kayıp değerlerinin grafiği\n",
        "plt.plot(total_loss_list, label='Total Loss')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Total Loss')\n",
        "plt.title('Total Loss Değişimi')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Style loss değerlerinin grafiği\n",
        "plt.plot(style_loss_list, label='Style Loss', color='orange')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Style Loss')\n",
        "plt.title('Style Loss Değişimi')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Content loss değerlerinin grafiği\n",
        "plt.plot(content_loss_list, label='Content Loss', color='green')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Content Loss')\n",
        "plt.title('Content Loss Değişimi')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "save_image(generated, \"generated.png\")"
      ],
      "metadata": {
        "id": "-FJeDNLttdel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "import numpy as np\n",
        "\n",
        "# Örnek olarak, orijinal ve üretilen görüntülerin numpy dizileri olarak temsili\n",
        "original_image_np = original_img.squeeze(0).cpu().detach().numpy().transpose(1, 2, 0)\n",
        "generated_image_np = generated.squeeze(0).cpu().detach().numpy().transpose(1, 2, 0)\n",
        "\n",
        "# SSIM hesaplama\n",
        "ssim_value = ssim(original_image_np, generated_image_np, multichannel=True)\n",
        "\n",
        "print(\"SSIM:\", ssim_value)\n"
      ],
      "metadata": {
        "id": "_shj-EeT9D6n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}